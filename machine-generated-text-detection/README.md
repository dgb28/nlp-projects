# ğŸ¤– Human vs Machine Text Classification

## ğŸŒŸ Overview
This project distinguishes **human-written** from **machine-generated text** using **bigram analysis**, **Logistic Regression**, and **Decision Trees**. It focuses on **feature importance**, **regularization effects**, and **model interpretability**.

## ğŸ”§ Methodology

### Dataset
- Text samples (train, dev, test)
- Features: Bag-of-bigrams (vocabulary 10kâ€“25k)

### Models & Hyperparameters

#### Logistic Regression
- C = 0.1, Penalty = L2, Max Iter = 500, Vocab = 25,000
- Train Accuracy: 0.9029  
- Dev Accuracy: 0.7424  
- Test Accuracy: 0.7087

#### Decision Tree
- Max Depth = 100, Min Samples Split = 500, Vocab = 10,000
- Train Accuracy: 0.7326  
- Dev Accuracy: 0.6554  
- Test Accuracy: 0.6283

## ğŸ” Key Insights
- **Human-like bigrams:** `('.', 'cmv')`, `('change', 'my')`, `('told', 'bbc')`
- **Machine-like bigrams:** `('read', 'more')`, `('per', 'cent')`, `('etc.', ',')`
- Punctuation and formal connectors are **important discriminators**.
- **Regularization effects:**
  - L1 promotes sparsity and highlights key bigrams.
  - L2 distributes weights more evenly for smoother generalization.
- Logistic Regression **generalizes better** than Decision Trees.

## ğŸŒ³ Decision Tree Example Paths

| Path | Splits | Leaf Prediction |
|------|--------|----------------|
| 1    | â€œa footnoteâ€ â†’ â€œsuch asâ€ â†’ â€œtheâ€ | Machine |
| 2    | â€œa footnoteâ€ â†’ â€œsuch asâ€ â†’ â€œurl1â€ | Human |
| 3    | â€œa footnoteâ€ â†’ False â†’ â€œ,â€ â†’ â€œdue toâ€ | Machine |

## ğŸ“Š Hyperparameter Summary

### Logistic Regression
| C    | Penalty | Max Iter | Vocab | Train Acc | Dev Acc |
|------|---------|----------|-------|-----------|---------|
| 0.1  | L2      | 500      | 25,000| 0.9029    | 0.7424 âœ… |

### Decision Tree
| Max Depth | Min Samples Split | Vocab | Train Acc | Dev Acc |
|-----------|-----------------|-------|-----------|---------|
| 100       | 500             | 10,000| 0.7326    | 0.6554 âœ… |

## ğŸš€ Future Work
- Implement **neural sequence models** like LSTM or Transformer to capture context.
- Explore **ensemble methods** combining Logistic Regression and Decision Trees.
- Visualize **bigram importance** for enhanced interpretability.
